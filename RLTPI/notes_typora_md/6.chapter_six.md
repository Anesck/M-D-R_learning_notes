## 第六章：函数近似（function approximation）方法

在有些任务中，状态和动作对的数目非常大，甚至可能是无穷大，这时不可能对所有状态（或状态动作对）逐一进行更新。函数近似方法用参数化的模型来近似整个状态价值函数（或动作价值函数），并在每次学习时更新整个函数。

### 一、函数近似原理

函数近似（function approximation）方法使用带参数 $\bold w$ 的函数来近似价值函数，如用 $v(s;\bold w),\; s \in \mathcal S$ 近似状态价值函数，用 $q(s,a;\bold w),\; s \in \mathcal S,a \in \mathcal A$ 近似动作价值函数。在动作集有限的情况下，还可以使用一个矢量函数 $q(s;\bold W)=(q(s,a;\bold w):a \in \mathcal A),\; s \in \mathcal S$ 来近似动作价值，矢量函数 $q(s;\bold w)$ 的每一个元素对应着一个动作，而整个矢量函数除参数外只用状态作为输入。

函数近似方法可以使用随机梯度下降算法或者半梯度下降算法对价值函数进行更新。以动作价值更新为例，**随机梯度下降**（stochastic gradient-descent, SGD）算法就是在试图减小每一步的回报估计 $G_t$ 和动作价值 $q(S_t,A_t;\bold w)$ 的差别时，定义每一步损失为 $[G_t-q(S_t,A_t,\bold w)]^2$ ，那么对整个回合的损失函数为 $\displaystyle \sum_{t=0}^{T-1}[G_t-q(S_t,A_t;\bold w)]^2$ ，然后再沿着回合损失函数对 $\bold w$ 的梯度反方向更新策略参数 $\bold w$ 。

对于能够支持自动梯度计算的软件包，往往自带根据损失函数更新参数的功能。同样也可以自己计算梯度 $\nabla q(S_t,A_t;\bold w)$ ，然后利用下式更新：
$$
\bold w \leftarrow \bold w - \frac{1}{2} \alpha_t \nabla[G_t - q(S_t,A_t;\bold w)]^2 = \bold w + \alpha_t [G_t - q(S_t,A_t;\bold w)] \nabla q(S_t,A_t;\bold w)
\label{eq:1}
$$
对状态价值函数也可以类似的定义回合损失函数 $\displaystyle \sum_{t=0}^{T}[G_t-v(S_t;\bold w)]^2$ ，其对应的更新式为：
$$
\bold w \leftarrow \bold w - \frac{1}{2} \alpha_t \nabla[G_t - v(S_t;\bold w)]^2 = \bold w + \alpha_t [G_t - v(S_t;\bold w)] \nabla v(S_t;\bold w)
\label{eq:2}
$$
将同策回合更新价值估计与函数近似法相结合，并在更新价值函数时使用随机梯度下降算法，就能得到算法 6-1 ：
$$
\; \\ \; \\
\large \textbf{算法 6-1   随机梯度下降函数近似评估策略的价值} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{1.（初始化）任意初始化参数 $\bold w$ 。} \\
&\text{2.（回合更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（采样）用策略 $\pi$ 生成轨迹 $S_0,A_0,R_1,S_1,\cdots,S_{T-1},A_{T-1},R_T,S_T$ 。} \\
&\qquad \text{2.2（初始化回报）$G \leftarrow 0$ 。} \\
&\qquad \text{2.3（逐步更新）对 $t \leftarrow T-1,T-2,\cdots,0$ ，执行以下步骤：} \\
&\qquad \qquad \text{2.3.1（更新回报）$G \leftarrow \gamma G + R_{t+1}$ ；} \\
&\qquad \qquad \text{2.3.2（更新价值）更新 $\bold w$ 以减小 $[G-q(S_t,A_t;\bold w)]^2$ 或 $[G-v(S_t;\bold w)]^2$ ，如式 $\eqref{eq:1}$ 或 $\eqref{eq:2}$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

将策略改进引入算法 6-1 即可实现最优策略求解算法 6-2 ：
$$
\; \\ \; \\
\large \textbf{算法 6-2   随机梯度下降求最优策略} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 6-1} \quad \cdots \\
&\text{2.1（采样）用 $q(\cdot,\cdot;\bold w)$ 导出策略（如 $\varepsilon$ 柔性策略）生成轨迹 $S_0,A_0,R_1,S_1,\cdots,S_{T-1},A_{T-1},R_T,S_T$ 。} \\
&\cdots \quad \text{同算法 6-1} \quad \cdots \\
&\qquad \text{2.3.2（更新价值）更新 $\bold w$ 以减小 $[G-q(S_t,A_t;\bold w)]^2$ ，如式 $\eqref{eq:1}$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

对于**半梯度下降**（semi-gradient descent）算法，就是在随机梯度下降算法的基础上，改为使用单步时序差分的回报估计 $U_t$ ，并且在对回合损失函数 $\displaystyle \sum_{t=0}^{T-1}[U_t-q(S_t,A_t;\bold w)]^2$ 或 $\displaystyle \sum_{t=0}^{T}[U_t-v(S_t;\bold w)]^2$ 求梯度时，不对回报 $U_t=R_{t+1} + \gamma q(S_{t+1},A_{t+1};\bold w)$ 或 $U_t=R_{t+1} + \gamma v(S_{t+1};\bold w)$ 求梯度。将半梯度下降算法与第五章节的算法相结合可以得到以下两个算法：
$$
\; \\ \; \\
\large \textbf{算法 6-3   半梯度下降算法估计动作价值或 SARSA 算法求最优策略} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{1.（初始化）任意初始化参数 $\bold w$ 。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态动作对）选择状态 $S$ ，用 $\pi(\cdot \mid S)$ 或 $q(S,\cdot;\bold w)$ 确定动作 $A$ 。} \\
&\qquad \text{2.2 $\;\,$若回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.2 $\;\,$用 $\pi(\cdot \mid S)$ 或 $q(S,\cdot;\bold w)$ 确定动作 $A'$ ；} \\
&\qquad \qquad \text{2.2.3（计算回报的估计值）$U \leftarrow R + \gamma q(S',A';\bold w)$ ；} \\
&\qquad \qquad \text{2.2.4（更新价值）更新 $\bold w$ 以减小 $[U-q(S,A;\bold w)]^2$ ；} \\
&\qquad \qquad \text{2.2.5 $\;\, S \leftarrow S',\; A \leftarrow A'$ 。}\\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

$$
\; \\ \; \\
\large \textbf{算法 6-4   半梯度下降算法估计状态价值或期望 SARSA 算法或 Q 学习} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{1.（初始化）任意初始化参数 $\bold w$ 。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态动作对）选择状态 $S$ 。} \\
&\qquad \text{2.2 $\;\,$若回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1 $\;\,$用 $\pi(\cdot \mid S)$ 或 $q(S,\cdot;\bold w)$ 确定动作 $A$ ；} \\
&\qquad \qquad \text{2.2.2（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.3（计算回报的估计值）状态价值评估：$U \leftarrow R + \gamma v(S';\bold w)$ ，期望 SARSA 算法：} \\
&\qquad \qquad \qquad \;\, \text{$U \leftarrow R + \gamma \sum_a \pi(a \mid S';\bold w) q(S',a;\bold w)$ ，其中 $\pi(\cdot \mid S';\bold w)$ 是 $q(S',\cdot;\bold w)$ 确定的} \\
&\qquad \qquad \qquad \;\, \text{策略（如 $\varepsilon$ 柔性策略），Q 学习：$U \leftarrow R + \gamma \max_a\, q(S',a;\bold w$）；} \\
&\qquad \qquad \text{2.2.4（更新价值）状态价值评估：更新 $\bold w$ 以减小 $[U-v(S;\bold w)]^2$ ，期望 SARSA 算法} \\
&\qquad \qquad \qquad \;\, \text{和 Q 学习：更新 $\bold w$ 以减小 $[U-q(S,A;\bold w)]^2$ ；}\\
&\qquad \qquad \text{2.2.5 $\;\, S \leftarrow S'$ 。}\\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

需要注意的是，当采用自动计算微分并更新参数的软件包来减小损失时，则务必注意不能对回报的估计求梯度。

资格迹同样可以运用在函数近似算法中，实现回合更新和单步时序差分的折中。这时
$$

$$

### 二、线性近似

**线性近似**是用许多特征向量的线性组合来近似价值函数，特征向量则依赖于输入（即状态或动作状态对），以动作价值近似为例，可以为每个状态动作对定义多个不同的特征 $\bold x(s,a)=(x_j(s,a:j \in \mathcal J))$ ，进而定义近似函数为这些特征的线性组合，即：
$$
q(s,a;\bold w)=[\bold x(s,a)]^T\bold w = \sum_{j \in \mathcal J} x_j(s,a)w_j \;\,, \qquad s \in \mathcal S,a \in \mathcal A
$$
对于状态函数也有类似的近似方法：
$$
v(s;\bold w)=[\bold x(s)]^T\bold w = \sum_{j \in \mathcal J} x_j(s)w_j \;\,, \qquad s \in \mathcal S
$$
 第三到五章介绍的查表法可以看做是线性近似的特例，例如对动作价值而言，可以认为有 $|\mathcal S| \times |\mathcal A|$ 个特征向量，每个向量形式为：
$$
\left(\underset{\underset{\huge{s,a}}{\large\uparrow}}{0, \cdots, 0, 1, 0, \cdots, 0}\right)
$$
即在某个的状态动作对处为 1 ，其他都为 0 。这样所有向量的线性组合就是整个动作价值函数，线性组合系数的值就是动作价值函数的值。

在使用线性近似的情况下，还可以使用线性最小二乘来进行策略评估。线性最小二乘是一种批处理（batch）方法，它每次针对多个经验样本，试图找到在整个样本集上最优的估计。将线性最小二乘用于回合更新，可以得到**线性最小二乘回合更新**（Linear Least Square Monte Carlo, Linear LSMC），它试图最小化目标：
$$
L(\bold w) = \sum_t [G_t - q(S_t, A_t; \bold w)]^2
$$


### 三、函数近似的收敛性

线性近似具有简单的线性叠加结构，这使得线性近似可以获得额外的收敛性

|      |      |      |      |      |
| ---- | ---- | ---- | ---- | ---- |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |
|      |      |      |      |      |



### 四、深度 Q 学习



### 五、案例：小车上山（MountainCar-v0）



