## 第五章：时序差分价值迭代

时序差分更新与回合更新的区别在于，时需差分更新汲取了动态规划方法中“自益”的思想，用现有的价值估计值来更新价值估计，不需要等到回合结束也可以更新价值估计。时序差分更新也包括同策和异策，同时还能分为单步更新和多步更新。

### 一、同策时序差分更新

从给定策略 $\pi$ 的情况下动作价值的定义出发，则有下式：
$$
\begin{equation}
\begin{split}
q_\pi(s,a) &= E_\pi (G_t \mid S_t=s,A_t=a) \\
&= E_\pi (R_{t+1} + \gamma G_{t+1} \mid S_t=s,A_t=a) \\
&= E_\pi (R_{t+1} + \gamma q_\pi(S_{t+1},A_{t+1}) \mid S_t=s,A_t=a) \qquad s \in \mathcal S, a \in \mathcal A(s) \\
\end{split}
\end{equation}
\label{eq:1}
$$
单步时序差分更新就是依据式 $\eqref{eq:1}$ 来更新的，因此只需采样一步，就可以使用 $U_t=R_{t+1} + \gamma q_\pi(S_{t+1}, \cdot)$ 来估计回报样本的值。为了与由奖励直接计算得到的无偏回报样本 $G_t$ 进行区别，使用字母 $U_t$ 表示使用自益得到的有偏回报样本。

根据以上分析，可以定义时序差分目标：

- 动作价值单步时序差分目标：$U_{t:t+1}^{(q)} = R_{t+1} + \gamma q(S_{t+1},A_{t+1})$ 
- 状态价值单步时序差分目标：$U_{t:t+1}^{(v)} = R_{t+1} + \gamma v(S_{t+1})$ 

其中 $U_{t:t+1}^{(q)}$ 的下标 $t:t+1$ 表示用 $(S_{t+1},A_{t+1})$ 的估计值来估计 $(S_t,A_t)$ 。再推广到多步时序差分目标：

- 动作价值 n 步时序差分目标：$U_{t:t+n}^{(q)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n q(S_{t+n},A_{t+n}) = R_{t+1}+\gamma U_{t+1:t+n}^{(q)}$ ，简写为 $U_t^{(q)}$ 或 $U_t$ 
- 状态价值 n 步时序差分目标：$U_{t:t+n}^{(v)} = R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n v(S_{t+n}) = R_{t+1}+\gamma U_{t+1:t+n}^{(v)}$ ，简写为 $U_t^{(v)}$ 或 $U_t$ 

在回合更新算法中是使用 $\displaystyle q(S_t,A_t) \leftarrow q(S_t,A_t) + \frac{1}{c(S_t,A_t)}[G_t-q(S_t,A_t)]$  增量更新来学习动作价值函数，而该式子在时序差分中，回报 $G_t$ 由 $U_t$ 替代；更新系数 $\displaystyle \frac{1}{c(S_t,A_t)}$ 被称为学习率，用 $\alpha \in (0,1]$ 替代。考虑到时序差分算法执行的过程中价值函数会越来越准确，进而基于价值函数估计得到的价值函数也会越来越准确，因此估计值的权重可以越来越大，所以算法中可以采用一个固定的学习率 $\alpha$ ，在有些问题中，让学习率巧妙地变化能得到更好的效果。

那么可以得到时序差分更新价值的表达式：
$$
q(S_t,A_t) \leftarrow q(S_t,A_t) + \alpha[U_t-q(S_t,A_t)] \\
v(S_t) \leftarrow v(S_t) + \alpha[U_t-v(S_t)]
\label{eq:2}
$$
根据表达式 $\eqref{eq:2}$ 可以得到单步时序差分更新评估策略的动作（状态）价值算法：
$$
\; \\ \; \\
\large \textbf{算法 5-1   单步时序差分更新评估策略的动作（或状态）价值} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述） ，策略 $\pi$ 。} \\
&\text{输出：价值函数 $q(s,a),\; s \in \mathcal S, a \in \mathcal A$ ，（或 $v(s),\; s \in \mathcal S$ ）。} \\
&\text{参数：优化器（隐含学习率 $\alpha$ ），折扣因子 $\gamma$ ，控制回合数和回合内步数的参数。} \\
&\text{1.（初始化）初始化价值估计 $q(s,a) \leftarrow$ 任意值，$s \in \mathcal S, a \in \mathcal A$ ，（或 $v(s) \leftarrow$ 任意值，$s \in \mathcal S$ ）。} \\
&\qquad \text{如果有终止状态，令 $q(s_终,a) \leftarrow 0,\; a \in \mathcal A$ ，（或 $v(s_终) \leftarrow 0$ ）。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态动作对（或状态））选择状态 $S$ ，再根据输入策略 $\pi$ 确定动作 $A$ ，（或选择状态 $S$ ）。} \\
&\qquad \text{2.2 $\;\,$如果回合未结束（如未达最大步数、$S' \ne s_终$ ），执行以下操作：} \\
&\qquad \qquad \text{2.2.1（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ，再用输入策略 $\pi$ 确定动作 $A'$ ，} \\
&\qquad \qquad \qquad \;\, \text{（或根据输入策略 $\pi$ 确定动作 $A$ ，再执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ）；} \\
&\qquad \qquad \text{2.2.2（计算回报的估计值）$U \leftarrow R + \gamma q(S',A')$ ，（或$U \leftarrow R + \gamma v(S')$ ）；} \\
&\qquad \qquad \text{2.2.3（更新价值）更新 $q(S,A)$（或 $v(S)$ ）以减小 $[U-q(S,A)]^2$（或 $[U-v(S)]^2$ ），} \\
&\qquad \qquad \qquad \;\, \text{如 $q(S,A) \leftarrow q(S,A) + \alpha[U-q(S,A)]$ ，（或 $v(S) \leftarrow v(S) + \alpha[U-v(S)]$ ）；} \\
&\qquad \qquad \text{2.2.4 $\;\, S \leftarrow S'$ ， $A \leftarrow A'$ ，（或 $S \leftarrow S'$ ）。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
在无模型的情况下，用回合更新和时序差分更新来评估策略都能渐近得到真实的价值函数，它们各有优劣，目前并没有证明某种方法就比另外一种方法更好。根据经验，学习率为常数的时序差分更新常常比学习率为常数的回合更新更快收敛，但时序差分更新对环境的 Markov 性要求更高。由于时序差分更新的 $U_t$ 使用了自益（下一个状态的样本 $(S_{t+1},A_{t+1})$ ），如果环境真的是 Markov 决策过程，那么自益能够更加有效的使用轨迹样本；若环境不是 Markov 决策过程，那么自益反而会导致错误的估计，而回合更新只使用了当前状态的样本，并不会产生错误的估计。

将单步时序差分算法推广到多步，即可得到算法 5-2 ：
$$
\; \\ \; \\
\large \textbf{算法 5-2   n 步时序差分更新评估策略的动作（或状态）价值} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \\
&\text{同算法 5-1 ，但加一个参数：步数 n 。} \\
&\cdots \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（采样）用 $\pi$ 生成轨迹 $S_0,A_0,R_1,\cdots,R_n,S_n$（令终止状态之后的奖励均为 0，状态均为 $S_终$ ）。} \\
&\qquad \text{2.2 $\;\,$ 对于 $t=0,1,2,\cdots$ 依次执行以下操作，直到 $S_t=s_终$：} \\
&\qquad \qquad \text{2.2.1（更新时序差分目标）$U_{t:t+n}^{(q)} \leftarrow R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n q(S_{t+n},A_{t+n})$ ，} \\
&\qquad \qquad \qquad \;\, \text{（或 $U_{t:t+n}^{(v)} \leftarrow R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n v(S_{t+n})$ ）；} \\
&\qquad \qquad \text{2.2.2（更新价值）更新 $q(S_t,A_t)$（或 $v(S_t)$ ）以减小 $[U-q(S_t,A_t)]^2$（或 $[U-v(S_t)]^2$ ）；} \\
&\qquad \qquad \text{2.2.3 $\;\,$若 $S_{t+n} \ne s_终$ ，则执行 $A_{t+n}$ （或根据 $\pi(\cdot \mid S_{t+n})$ 决定动作 $A_{t+n}$ 并执行），得到奖励} \\
&\qquad \qquad \qquad \;\, \text{$R_{t+n+1}$ 和下一状态 $S_{t+n+1}$ ；若 $S_{t+n} = s_终$ ，令 $R_{t+n+1} \leftarrow 0$ ，$S_{t+n+1} \leftarrow s_终$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
在评估策略的价值后进行策略改进，可以得到使用同策时序差分更新求解最优策略的“状态 / 动作 / 奖励 / 状态 / 动作”（State-Action-Reward-State-Action，SARSA）算法；其中策略的提升方法可以采用 $\varepsilon$ 贪心算法，使得 $\pi$ 总是柔性策略，且中间过程的策略既可以显式更新和存储，也可以不显式存储：
$$
\; \\ \; \\
\large \textbf{算法 5-3   SARSA 算法求解最优策略} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述）。} \\
&\text{输出：最优策略估计 $\pi(a \mid s)$ 和最优动作价值估计 $q(s,a)$ ，$s \in \mathcal S, a \in \mathcal A$ 。} \\
&\text{参数：优化器（隐含学习率 $\alpha$ ），折扣因子 $\gamma$ ，策略改进的参数（如 $\varepsilon$ ），控制回合数和回合内步数的参数。} \\
&\text{1.（初始化）$q(s,a) \leftarrow$ 任意值，如果有终止状态，令 $q(s_终,a) \leftarrow 0$ ，$s \in \mathcal S, a \in \mathcal A(s)$ ；如果是显式更新} \\
&\qquad \text{和存储策略，则用动作价值 $q(s,a)$ 确定策略 $\pi$（如 $\varepsilon$ 贪心策略）。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态动作对）选择状态 $S$ ，用动作价值 $q(S,\cdot)$ 确定的策略决定动作 $A$（如 $\varepsilon$ 贪心策略）；} \\
&\qquad \qquad \text{若为显式存储策略，则用策略 $\pi$ 确定动作 $A$ 。} \\
&\qquad \text{2.2 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.2 $\;\,$用 $q(S',\cdot)$ 确定的策略决定动作 $A'$ ；若为显式存储策略，则用策略 $\pi$ 确定动作 $A'$ ；} \\
&\qquad \qquad \text{2.2.3（计算回报的估计值）$U \leftarrow R + \gamma q(S',A')$ ，（或$U \leftarrow R + \gamma v(S')$ ）；} \\
&\qquad \qquad \text{2.2.4（更新价值）更新 $q(S,A)$ 以减小 $[U-q(S,A)]^2$ ；} \\
&\qquad \qquad \text{2.2.5（若为显示存储策略则进行策略改进）根据 $q(S,\cdot)$ 修改 $\pi(\cdot \mid S)$（如 $\varepsilon$ 贪心策略）；} \\
&\qquad \qquad \text{2.2.6 $\;\, S \leftarrow S'$ ， $A \leftarrow A'$ 。} \\
&\qquad \text{2.3（若不为显式存储策略）由最优动作价值估计 $q(s,a)$ 确定最优策略估计 $\pi(a \mid s)$ ，$s \in \mathcal S, a \in \mathcal A$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
在 SARSA 算法中采用多步时序差分目标，即可得到多步 SARSA 算法，类似于从算法 5-1 到 5-2 ，该算法可由算法 5-3 修改得到，此处略。

SARSA 算法有一种变换——**期望 SARSA 算法**（Expected SARSA），它的不同之处在于，在估计 $U_t$ 时，不使用基于动作价值的时序差分目标，而使用基于状态价值的时序差分目标，再利用 Bellman 方程有：
$$
U_{t:t+1}^{(v)} = R_{t+1} + \gamma v(S_{t+1}) = R_{t+1} + \gamma \sum_{a \in \mathcal A(S_{t+1})} \pi(a \mid S_{t+1})q(S_{t+1},a)
$$
该式需要计算 $\sum_a\pi(a \mid S_{t+1})q(S_{t+1},a)$ ，虽然计算量增大，但这样的期望运算减小了 SARSA 算法中出现的个别不恰当决策，因此期望 SARSA 常常有比 SARSA 更大的学习率，在很多情况下，期望 SARSA 的效果会比 SARSA 稍微好一些。算法 5-4 给出了期望 SARSA 算法：
$$
\; \\ \; \\
\large \textbf{算法 5-4   期望 SARSA 求解最优策略} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{1.（初始化） $q(s,a) \leftarrow$ 任意值，如果有终止状态，令 $q(s_终,a) \leftarrow 0$ ，$s \in \mathcal S, a \in \mathcal A$ ；} \\
&\qquad \text{再用动作价值 $q(s,a)$ 确定策略 $\pi$（如 $\varepsilon$ 贪心策略）。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态）选择状态 $S$ 。} \\
&\qquad \text{2.2 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1 $\;\,$用动作价值 $q(S,\cdot)$ 确定的策略 $\pi$ 来确定动作 $A$ ；} \\
&\qquad \qquad \text{2.2.2（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.3（用期望计算回报的估计值）$U \leftarrow R + \gamma \sum_{a \in \mathcal A(S')} \pi(a \mid S') q(S',a)$ ；} \\
&\qquad \qquad \text{2.2.4（更新价值）更新 $q(S,A)$ 以减小 $[U-q(S,A)]^2$ ；} \\
&\qquad \qquad \text{2.2.5 $\;\, S \leftarrow S'$。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
期望 SARSA 算法同样也有多步版本，与前面的多步算法类似，可由单步版本修改得到，此处略。

### 二、异策时序差分更新

前面已经介绍过异策算法是基于重要性采样的，将该方法整合到时序差分策略评估动作价值算法或 SARSA 算法中，即可得到它们的重要性采样版本，算法 5-5 给出了多步时序差分的版本：
$$
\; \\ \; \\
\large \textbf{算法 5-5   重要性采样 n 步时序差分策略评估动作价值或 SARSA 算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述） ，策略 $\pi$ 。} \\
&\text{输出：价值函数 $q(s,a),\; s \in \mathcal S, a \in \mathcal A(s)$ ，若是最优策略控制则还要输出策略 $\pi$ 。} \\
&\text{参数：步数 n ，优化器（隐含学习率 $\alpha$ ），折扣因子 $\gamma$ ，控制回合数和回合内步数的参数。} \\
&\text{1.（初始化） $q(s,a) \leftarrow$ 任意值，如果有终止状态，令 $q(s_终,a) \leftarrow 0$ ，$s \in \mathcal S, a \in \mathcal A$ ；若是最优策略控制，} \\
&\qquad \text{还应用 $q(s,a)$ 确定策略 $\pi$（如 $\varepsilon$ 贪心策略）。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（行为策略）指定行为策略 $b$ 使得 $\pi \ll b$ 。} \\
&\qquad \text{2.2（采样）用 $b$ 生成轨迹 $S_0,A_0,R_1,\cdots,R_n,S_n$（令终止状态之后的奖励均为 0，状态均为 $S_终$ ）} \\
&\qquad \text{2.3 $\;\,$ 对于 $t=0,1,2,\cdots$ 依次执行以下操作，直到 $S_t=s_终$：} \\
&\qquad \qquad \text{2.3.1 $\;\,$若 $S_{t+n} \ne s_终$ ，则根据 $\pi(\cdot \mid S_{t+n})$ 决定动作 $A_{t+n}$ ；} \\
&\qquad \qquad \text{2.3.2（更新时序差分目标）$U_{t:t+n}^{(q)} \leftarrow R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n q(S_{t+n},A_{t+n})$ ;} \\
&\qquad \qquad \text{2.3.3（计算重要性采样比率）$\rho_{t+1:t+n-1} \leftarrow \prod_{\tau=t+1}^{\min\{t+n,T\}-1} \frac{\pi(A_\tau \mid S_\tau)}{b(A_\tau \mid S_\tau)}$ ；} \\
&\qquad \qquad \text{2.3.4（更新价值）更新 $q(S_t,A_t)$ 以减小 $\rho [U-q(S_t,A_t)]^2$ ；} \\
&\qquad \qquad \text{2.3.5（更新策略）如果是最优策略求解算法，需要根据 $q(S,\cdot)$ 修改 $\pi(\cdot \mid S)$ ；} \\
&\qquad \qquad \text{2.3.6 $\;\,$若 $S_{t+n} \ne s_终$ ，则执行 $A_{t+n}$ ，得到奖励 $R_{t+n+1}$ 和下一状态 $S_{t+n+1}$ ；若 $S_{t+n} = s_终$ ，} \\
&\qquad \qquad \qquad \;\, \text{则令 $R_{t+n+1} \leftarrow 0$ ，$S_{t+n+1} \leftarrow s_终$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
单步版本也可以由前面的算法修改得到，另外还可以类似的方法将重要性采样运用到时序差分状态价值估计和期望 SARSA 算法中，此处略。

异策时序差分更新是比同策时序差分更新更加流行的算法，特别是 Q 学习算法，已经成为最重要的基础算法之一。该算法是从改进后策略的行为出发，将时序差分目标改为 $\displaystyle U_t = R_{t+1} + \gamma \max_{a \in \mathcal A(S_{t+1})} q(S_{t+1},a)$ ，即直接使用根据 $q(S_{t+1},\cdot)$ 改进后的策略来更新，使得可以更接近最优价值。因此 Q 学习的更新式不是基于当前的策略，而是基于另外一个不一定要使用的确定性策略来更新动作价值，从这个意义上来看，Q 学习是一个异策算法：
$$
\; \\ \; \\
\large \textbf{算法 5-6   Q 学习算法求解最优策略} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{1.（初始化） $q(s,a) \leftarrow$ 任意值，如果有终止状态，令 $q(s_终,a) \leftarrow 0$ ，$s \in \mathcal S, a \in \mathcal A$ 。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态）选择状态 $S$ 。} \\
&\qquad \text{2.2 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1 $\;\,$用动作价值 $q(S,\cdot)$ 确定的策略 $\pi$ 来确定动作 $A$（如 $\varepsilon$ 贪心策略） ；} \\
&\qquad \qquad \text{2.2.2（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.3（用改进后的策略计算回报的估计值）$U \leftarrow R + \gamma \max_{a \in \mathcal A(S')} q(S',a)$ ；} \\
&\qquad \qquad \text{2.2.4（更新价值和策略）更新 $q(S,A)$ 以减小 $[U-q(S,A)]^2$ ；} \\
&\qquad \qquad \text{2.2.5 $\;\, S \leftarrow S'$。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
Q 学习也有多步版本，其目标为 $\displaystyle U_t= R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1}R_{t+n} + \gamma^n \max_{a \in \mathcal A(S_{t+n})} q(S_{t+n},a)$ ，此处略。

由于 Q 学习使用 $\max_aq(S_{t+1},a)$ 来更新动作价值，当奖励存在偏差时，该方法会导致“最大化偏差”（maximization bias），使得估计的动作价值偏差较大。为解决这一问题，**双重 Q 学习**（Double Q Learning） 算法使用两个独立的动作价值估计值 $q^{(0)}(\cdot,\cdot)$ 和 $q^{(1)}(\cdot,\cdot)$ ，用 $q^{(0)}(S_{t+1},\arg\max_a q^{(1)}(S_t+1,a))$ 或 $q^{(1)}(S_{t+1},\arg\max_a q^{(0)}(S_t+1,a))$ 来代替 Q 学习中的  $\max_aq(S_{t+1},a)$ 。由于 $q^{(0)}$ 和 $q^{(1)}$ 是相互独立的估计，所以 $E[q^{(0)}(S_t+1,A^*)]=q(S_{t+1},A^*)$ ，其中 $A^*=\arg\max_a q^{(1)}(S_{t+1},a)$ ，这样就消除了偏差。 $q^{(0)}$ 和 $q^{(1)}$ 都需要逐渐更新，一种方法是在每步的学习中以等概论选择两个更新中的任意一个：
$$
\; \\ \; \\
\large \textbf{算法 5-7   双重 Q 学习算法求解最优策略} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{1.（初始化） $q^{(i)}(s,a) \leftarrow$ 任意值，如果有终止状态，令 $q^{(i)}(s_终,a) \leftarrow 0$ ，$s \in \mathcal S, a \in \mathcal A, i \in \{0,1\}$ 。} \\
&\text{2.（时序差分更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化状态）选择状态 $S$ 。} \\
&\qquad \text{2.2 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1 $\;\,$用动作价值 $(q^{(0)}+q^{(1)})(S,\cdot)$ 确定的策略 $\pi$ 来确定动作 $A$（如 $\varepsilon$ 贪心策略） ；} \\
&\qquad \qquad \text{2.2.2（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.3（随机选择更新）以等概率选择 $q^{(0)}$ 或 $q^{(1)}$ 作为更新对象，记选中的为 $q^{(i)},i \in \{0,1\}$ ；} \\
&\qquad \qquad \text{2.2.4（用改进后的策略计算回报的估计值）$U \leftarrow R + \gamma q^{(1-i)}(S',\underset{a}{\arg\max}\; q^{(i)}(S',a))$ ；} \\
&\qquad \qquad \text{2.2.5（更新价值）更新 $q^{(i)}(S,A)$ 以减小 $[U-q^{(i)}(S,A)]^2$ ；} \\
&\qquad \qquad \text{2.2.6 $\;\, S \leftarrow S'$。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
该算法最终输出的动作价值函数是 $q^{(0)}$ 和 $q^{(1)}$ 的平均值，但在中间步骤是直接使用  $q^{(0)}+q^{(1)}$ 的，该简化计算并不影响结果。

### 三、资格迹

资格迹是一种让时序差分学习更加有效的机制，它能够在回合更新和单步时序差分更新之间折中，并且实现简单，运行有效。在介绍资格迹之前，先介绍以下内容：

给定 $\lambda \in [0,1]$ ，$\pmb\lambda$ **回报**（$\lambda$ return）是时序差分目标 $U_{t:t+1},U_{t:t+2},\cdots$ 按 $(1-\lambda),(1-\lambda)\lambda,\cdots$ 加权平均的结果。对于连续性任务有：
$$
U_t^\lambda = (1-\lambda)\sum_{n=1}^{+\infty} \lambda^{n-1}U_{t:t+n}
$$

对于回合制任务，由于当 $t+n \ge T$ 时，$U_{t:t+n}=G_t$ ，故有：
$$
U_t^\lambda = (1-\lambda)\sum_{n=1}^{T-t-1} \lambda^{n-1}U_{t:t+n}+\lambda^{T-t-1}G_t
\label{eq:5}
$$
上式可以看做是回合更新目标 $G_t$ 和单步时序差分目标 $U_{t:t+1}$ 的推广：当 $\lambda=1$ 时，$U_t^1 = G_t$ 就是回合更新的回报；当 $\lambda=0$ 时，$U_t^0=U_{t:t+1}$ 就是单步时序差分目标。

**离线 $\pmb\lambda$ 回报算法**（offline $\lambda$-return algorithm）是以 $U_t^\lambda$ 作为目标，试图减小 $[U_t^\lambda-q(S_t,A_t)]^2$ 或 $$[U_t^\lambda-v(S_t)]^2$$ 的。对于回合制任务，该算法在回合结束后为每一步 $t=0,1,2,\cdots$ 计算 $U_t^\lambda$ ，并统一更新价值，因此这样的算法称为**离线算法**（offline algorithm）；由于使用的目标在 $G_t$ 和 $U_{t:t+1}$ 之间做了折中，所以其效果可能比单独使用这两个目标都要好。

但是离线 $\lambda$ 算法也有明显的缺点：

- 由于连续性任务无法计算 $U_t^\lambda$ ，所以无法使用该算法；
- 在回合结束后要计算 $U_t^\lambda$ ，计算量巨大。

采用资格迹可以弥补这两个缺点，$TD(\lambda)$ 是历史上具有重要影响力的强化学习算法，在离线 $\lambda$ 回报算法的基础上改进而来。事实上，在离线 $\lambda$ 回报算法中，知道 $(S_t,A_t)$ 后就能计算 $U_{t-n:t}$ 并部分更新 $q(S_{t-n},A_{t-n})$ ，$n=1,2,\cdots,t$ ，那么问题就转化为如何求得更新权重了。根据该思想引入资格迹 $e_t(s,a)$ 来表示第 $t$ 步的单步自益结果 $U_{t:t+1}$ 对每个状态动作对 $(s,a)$ ，$s \in \mathcal S, a \in \mathcal A(s)$ 需要更新的权重，**资格迹**（eligibility）定义如下：
$$
e_t(s,a)=
\left \{
\begin{aligned}
\begin{split}
&0, &t=0 \\
&1 + \beta\gamma\lambda e_{t-1}(s,a) \qquad &S_t=s,A_t=a \\
&\gamma\lambda e_{t-1}(s,a) &其他 \qquad
\end{split}
\end{aligned}
\right .
$$
其中 $\beta \in [0,1]$ 是事先给定的参数，表示对当前最新出现的状态动作对 $(S_t,A_t)$ ，它的更新权重则要进行某种强化，其取值常有以下几种：

- **累积迹**（accumulating trace）：$\beta=1$ ；
- **荷兰迹**（dutch trace）：$\beta=1-\alpha$ ，其中 $\alpha$ 为学习率；
- **替换迹**（replacing trace）：$\beta=0$ 。

对于资格迹的表达式可以这么理解：从 $\lambda$ 回报的各项 $\eqref{eq:6}$ 中可以看到，$U_\tau^\lambda$ 最大权重 $(1-\lambda)$ 是当前步的单步时序差分，因此对于当前的动作状态对，可以进行某种强化；而对于历史上的某个状态动作对 $(S_\tau,A_\tau)$ ，$U_{\tau:t}$ 在 $U_\tau^\lambda$ 的权重为 $(1-\lambda)\lambda^{t-\tau-1}$ ，因为 $U_{\tau:t}=R_{\tau+1}+\cdots+\gamma^{t-\tau-1}U_{t-1:t}$ ，所以单步时序差分 $U_{t-1:t}$ 是以 $(1-\lambda)(\lambda\gamma)^{t-\tau-1}$ 的比率折算到  $U_\tau^\lambda$ 中的，即间隔步数每增加一步，原先的资格迹大致需要衰减 $\gamma\lambda$ 倍。*（总的来说，对于历史记录越久的状态动作对，资格迹越小，更新力度越小；历史记录越新的状态动作对，资格迹越大，更新力度越大。参考资料：[莫烦python：什么是 Sarsa(lambda)][1]）*
$$
\begin{equation}
\begin{split}
&U_0^\lambda 的项 &U_1^\lambda 的项 &\cdots &U_{n-1}^\lambda 的项 \\
n=1: \quad &(1-\lambda) U_{0:1} \\
n=2: \quad &(1-\lambda)\lambda U_{0:2} &(1-\lambda)U_{1:2} & \\
\vdots \\
n: \quad &(1-\lambda)\lambda^{n-1} U_{0:n} \quad &(1-\lambda)\lambda^{n-2}U_{1:n} \quad &\cdots \quad &(1-\lambda) U_{n-1:n} \\
\end{split}
\end{equation}
\label{eq:6}
$$
资格迹也可用于状态价值，将动作价值资格迹内容中的动作状态对改为状态即可，此处略。利用资格迹，可以得到 $TD(\lambda)$ 策略评估算法，它是在单步时序差分的基础上，加入资格迹来实现的；资格迹也可以和最优策略求解算法结合，例如和 SARSA 算法结合得到 SARSA($\lambda$) 算法。算法 5-8 给出了 TD($\lambda$) 的动作价值评估或 SARSA($\lambda$) 学习算法，TD($\lambda$) 的状态价值评估可结合算法 5-1 修改得到，此处略。
$$
\; \\ \; \\
\large \textbf{算法 5-8   TD($\lambda$) 的动作价值评估或 SARSA($\lambda$) 学习} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述） ，若评估动作价值则需输入策略 $\pi$ 。} \\
&\text{输出：动作价值估计 $q(s,a),\; s \in \mathcal S, a \in \mathcal A$ 。} \\
&\text{参数：资格迹参数 $\beta$ ，优化器（隐含学习率 $\alpha$ ），折扣因子 $\gamma$ ，控制回合数和回合内步数的参数。} \\
&\text{1.（初始化） $q(s,a) \leftarrow$ 任意值，如果有终止状态，令 $q(s_终,a) \leftarrow 0$ ，$s \in \mathcal S, a \in \mathcal A$ 。} \\
&\text{2. $\;\,$对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化资格迹）$e(s,a) \leftarrow 0,\; s \in \mathcal S,a \in \mathcal A$ 。} \\
&\qquad \text{2.1（初始化状态动作对）选择状态 $S$ ，再根据策略 $\pi$ 或动作价值 $q$ 确定动作 $A$ 。} \\
&\qquad \text{2.2 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.2.1（采样）执行动作 $A$ ，观测得到的奖励 $R$ 和新状态 $S'$ ；} \\
&\qquad \qquad \text{2.2.2 $\;\,$根据策略 $\pi$ 动作价值 $q$ 确定动作 $A'$ ；} \\
&\qquad \qquad \text{2.2.3（更新资格迹）$e(s,a) \leftarrow \gamma\lambda e(s,a),\; s\in \mathcal S, a \in \mathcal A$ ，$e(S,A) \leftarrow 1 + \beta e(S,A)$ ；} \\
&\qquad \qquad \text{2.2.4（计算回报的估计值）$U \leftarrow R + \gamma q(S',A')$ ；} \\
&\qquad \qquad \text{2.2.5（更新价值）$q(s,a) \leftarrow q(s,a) + \alpha e(s,a)[U-q(s,a)],\; s \in \mathcal S, a \in \mathcal A$ ；} \\
&\qquad \qquad \text{2.2.6 $\;\, S \leftarrow S'$ ， $A \leftarrow A'$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
*（资格迹更新与离线 $\lambda$ 回报更新比较：假设 $q(s,a)$ 全部初始化为 0 ，使用替换迹，$S_0 \ne S_1$ ，考虑两步，则有*
$$
\begin{split}
t=0，&U_{0:1} = R_1&+\gamma q(S_1,A_1)\; 更新\; q(S_0,A_0)\; 以及其他所有动作状态对，有 \\
&q(S_0,A_0) &= q(S_0,A_0)+\alpha(U_{0:1}-q(S_0,A_0)) = \alpha U_{0:1}； \\
t=1，&U_{1:2} = R_2&+\gamma q(S_2,A_2)\; 更新\; q(S_0,A_0)\; 以及其他所有动作状态对，有 \\ 
&q(S_0,A_0) &= q(S_0,A_0) + \alpha\lambda\gamma(U_{1:2}-q(S_0,A_0)) \\
&&= (1-\alpha\lambda\gamma)q(S_0,A_0) + \alpha\lambda\gamma U_{1:2} \\
&& = (1-\alpha\lambda\gamma)\alpha U_{0:1} + \alpha\lambda(U_{0:2}-R_1) \\
&& = \alpha[(1-\alpha\lambda\gamma)U_{0:1} + \lambda(U_{0:2}-R_1)]\\
&& = \alpha[(1-\alpha\lambda\gamma)R_1 + \lambda\gamma R_2 + (1-\alpha\lambda\gamma)\gamma q(S_1,A_1) + \lambda\gamma^2 q(S_2,A_2)] \\ \\
离线 \lambda 回报： &q(S_0,A_0) &=q(S_0,A_0) + \alpha(U_0^\lambda -q(S_0,A_0)) \\
&&=\alpha[(1-\lambda)U_{0:1} + (1-\lambda)\lambda U_{0:2}] \\
&&=\alpha[(1-\lambda^2)R_1 + (1-\lambda)\lambda\gamma R_2 + (1-\lambda)\gamma q(S_1,A_1) + (1-\lambda)\lambda\gamma^2 q(S_2,A_2)]
\end{split}
$$
*从两者的展开计算结果可以看到，所有的加权项是相同的，只是加权权重不同。）*

### 四、案例：出租车调度（Taxi-v3）



[1]: https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-sarsa-lambda/