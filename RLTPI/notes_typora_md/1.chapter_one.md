## 第一章：初识强化学习

### 一、强化学习的基本概念：

- **奖励（reward）：**奖励是强化学习系统的学习目标。智能体在采取行动后会收到环境发来的奖励，而强化学习的目标就是要最大化长时间里的总奖励。
- **策略（policy）：**策略为强化学习的学习对象。策略会指导智能体根据当前环境来采取动作，策略可以是确定性的，也可以是不确定性的（概率分布），强化学习通过改进策略来最大化总奖励。
- **智能体（agent）：**强化学习系统中的行动者和学习者，它可以做出决策和接受奖励信号，我们并不需要对智能体本身进行建模，只需了解它在不同环境下可以做出的动作，并接受奖励信号。
- **环境（environment）：**强化学习系统中除智能体以外的所有事物，它是智能体交互的对象。环境可以是已知的，也可以是未知的，因此可以对环境建模，也可以不对环境建模。

### 二、智能体和环境交互的过程：

```mermaid
graph RL
subgraph 智能体/环境接口
A((*** 智能体 ***))
B((**** 环境 ****))
A == 动作 A ==>B
B == 观测 O ==>A
B == 奖励 R ==>A
end
```

*本人认为稍微具体点，可以展开如下：*

```mermaid
graph LR
A1 == 动作 A ==> B1
B1 == 观测 O ==> A3
B2 == 奖励 R ==> A3
subgraph 智能体
A1(决策部分)
A2(学习部分)
A3(接收部分)
A2 -- 改进 --> A1
A3 -- 有用信息 --> A2
A3 -- 观测 O --> A1
end
subgraph 环境
B1(当前状态)
B2(下一个状态)
B1 -- 改变至 --> B2
end
```

以上智能体/环境接口中，智能体和环境的交互主要有三个环节：
1. 智能体观测环境，可以获得环境的**观测（observation）**，一般记为 ***O***；
2. 智能体根据观测做出决策，决定要对环境施加的**动作（action）**，记为 ***A***；
3. 环境受智能体动作的影响，改变自己的**状态（state）**，记为 ***S***；并给出**奖励（reward）**，记为 ***R***

### 三、强化学习的分类：

```mermaid
graph LR
A((强化学习))
B(按任务分类)
C(按算法分类)
b1(单/多智能体任务)
b2(回合制/连续性任务)
b3(离散/连续时间环境)
b4(离散/连续动作空间)
b5(确定性/非确定性环境)
b6(完全/非完全可观测环境)
c1(同策/异策学习)
c2(有模型/无模型学习)
c3(回合/时序差分更新)
c4(基于价值/策略)
c5(深度/非深度强化学习算法)
B === A
A === C
b1 & b2 & b3 & b4 & b5 & b6 --- B
C --- c1 & c2 & c3 & c4 & c5
```

**注：**同策学习（on-policy）是边决策边学习，学习者同时也是决策者。异策学习（off-policy）则是通过之前的历史（可是自己的也可以是别人的）进行学习，学习者和决策者不需要相同。

