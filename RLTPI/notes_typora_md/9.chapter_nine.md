## 第九章：连续动作空间的确定性策略

为处理连续动作空间的情况，D. Silver 等在文章《Deterministic Policy Gradient Algorithms》中提出了确定性策略的方法。

对于连续动作空间里的确定性策略，$\pi(a \mid s;\theta)$ 并不是通常意义上的函数，它对策略参数 $\theta$ 的梯度 $\nabla \pi(a \mid s;\theta)$ 也不复存在。第二章曾提到确定性策略可以表示为 $\pi(s;\theta),\;(s \in \mathcal S)$ ，这种表示可以绕过由于 $\pi(a \mid s;\theta)$ 并不是通常意义上的函数而带来的困难。

当策略是一个连续动作空间上的确定性策略 $\pi(s;\theta),\;(s \in \mathcal S)$ 时，策略梯度定理为：
$$
\nabla E_{\pi(\theta)}[G_0] = E\left[\sum_{t=0}^{+\infty} \gamma^t \nabla\pi(S_t;\theta)\left[\nabla_a q_{\pi(\theta)}(S_t,a)\right]_{a=\pi(S_t;\theta)}\right]
\label{eq:1}
$$

证明：确定性策略的状态价值和动作价值满足以下关系：
$$
\begin{split}
v_{\pi(\theta)}(s) &= q_{\pi(\theta)}(s, \pi(s;\theta))\;, & s \in \mathcal S \\
q_{\pi(\theta)}(s, \pi(s;\theta)) &= r(s, \pi(s;\theta)) + \gamma\sum_{s'} p(s' \mid s, \pi(s;\theta)) v_{\pi(\theta)}(s')\;, \qquad & s \in \mathcal S
\end{split}
$$
以上两式对 $\theta$ 求梯度，有：
$$
\begin{split}
& \nabla v_{\pi(\theta)}(s) = \nabla q_{\pi(\theta)}(s, \pi(s;\theta))\;,\quad s \in \mathcal S \\
& \nabla q_{\pi(\theta)}(s, \pi(s;\theta)) = \big[\nabla_a r(s,a)\big]_{a=\pi(s;\theta)} \nabla\pi(s;\theta) + \\
&\qquad \gamma\sum_{s'} \left\{ \big[\nabla_ap(s' \mid s,a)\big]_{a=\pi(s;\theta)} \big[\nabla\pi(s;\theta)\big] v_{\pi(\theta)}(s') + p(s' \mid s, \pi(s;\theta))\nabla v_{\pi(\theta)}(s') \right\} \\
&= \nabla\pi(s;\theta) \left[\nabla_a r(s,a) + \gamma\sum_{s'} \nabla_ap(s' \mid s,a) v_{\pi(\theta)}(s')\right]_{a=\pi(s;\theta)} + \gamma\sum_{s'}p(s' \mid s, \pi(s;\theta))\nabla v_{\pi(\theta)}(s') \\
&= \nabla\pi(s;\theta) \big[\nabla_a q_{\pi(\theta)}(s, a)\big]_{a=\pi(s;\theta)} + \gamma\sum_{s'}p(s' \mid s, \pi(s;\theta))\nabla v_{\pi(\theta)}(s')\;,\quad s \in \mathcal S
\end{split}
$$
将 $\nabla q_{\pi(\theta)}(s, \pi(s;\theta))$ 的表达式代入 $\nabla v_{\pi(\theta)}(s)$ 的表达式中，再对 $\nabla v_{\pi(\theta)}(s)$ 求关于 $S_t$ 的期望，并考虑到 $p(s'\mid s,\pi(s;\theta)) = {\rm Pr}[S_{t+1}=s' \mid S_t=s;\pi(\theta)]$（其中 $t$ 任取），有：
$$
\begin{split}
& {\rm E}[\nabla v_{\pi(\theta)}(S_t)] = \sum_s {\rm Pr}[S_t=s] \nabla v_{\pi(\theta)}(S_t) \\
&= \sum_s{\rm Pr}[S_t=s] \left[\nabla\pi(s;\theta) \big[\nabla_a q_{\pi(\theta)}(s, a)\big]_{a=\pi(s;\theta)} + \gamma\sum_{s'}p(s' \mid s, \pi(s;\theta))\nabla v_{\pi(\theta)}(s') \right] \\
&= \sum_s{\rm Pr}[S_t=s] \left[\nabla\pi(s;\theta) \big[\nabla_a q_{\pi(\theta)}(s, a)\big]_{a=\pi(s;\theta)} + \gamma\sum_{s'}{\rm Pr}[S_{t+1}=s' \mid S_t=s;\pi(\theta)]\nabla v_{\pi(\theta)}(s') \right] \\
&= \sum_s{\rm Pr}[S_t=s] \nabla\pi(s;\theta) \big[\nabla_a q_{\pi(\theta)}(s, a)\big]_{a=\pi(s;\theta)} + \gamma\sum_s{\rm Pr}[S_t=s]\sum_{s'}{\rm Pr}[S_{t+1}=s' \mid S_t=s;\pi(\theta)]\nabla v_{\pi(\theta)}(s') \\
&= \sum_s{\rm Pr}[S_t=s] \nabla\pi(s;\theta) \big[\nabla_a q_{\pi(\theta)}(s, a)\big]_{a=\pi(s;\theta)} + \gamma\sum_{s'}{\rm Pr}[S_{t+1}=s';\pi(\theta)] \nabla v_{\pi(\theta)}(s') \\
&= {\rm E} \left[\nabla\pi(S_t;\theta)\big[\nabla_a q_{\pi(\theta)}(S_t, a)\big]_{a=\pi(S_t;\theta)} \right] + \gamma{\rm E}[\nabla v_{\pi(\theta)}(S_{t+1})]
\end{split}
$$
这样就得到了 ${\rm E}[\nabla v_{\pi(\theta)}(S_t)]$ 到 ${\rm E}[\nabla v_{\pi(\theta)}(S_{t+1})]$ 的递推式，然后对于最终关注的梯度值 $\nabla {\rm E}_{\pi_{(\theta)}}[G_0]$ 有：
$$
\begin{split}
& \nabla {\rm E}_{\pi_{(\theta)}}[G_0] = {\rm E}[\nabla v_{\pi(\theta)}(S_0)] \\
&= {\rm E} \left[\nabla\pi(S_0;\theta)\big[\nabla_a q_{\pi(\theta)}(S_0, a)\big]_{a=\pi(S_0;\theta)} \right] + \gamma{\rm E}[\nabla v_{\pi(\theta)}(S_1)] \\
&= {\rm E} \left[\nabla\pi(S_0;\theta)\big[\nabla_a q_{\pi(\theta)}(S_0, a)\big]_{a=\pi(S_0;\theta)} \right] + \gamma{\rm E} \left[\nabla\pi(S_1;\theta)\big[\nabla_a q_{\pi(\theta)}(S_1, a)\big]_{a=\pi(S_1;\theta)} \right]  + \gamma^2{\rm E}[\nabla v_{\pi(\theta)}(S_2)] \\
&= \cdots = \sum_{t=0}^{+\infty}{\rm E}\left[\gamma^t\nabla\pi(S_t;\theta)\big[\nabla_a q_{\pi(\theta)}(S_t, a)\big]_{a=\pi(S_t;\theta)} \right]=\eqref{eq:1}\text{式右边，证毕。}
\end{split}
$$
对于连续动作空间中的确定性策略，更常使用的是另外一种形式：
$$
\nabla E_{\pi(\theta)}[G_0] = E_{S\sim{\large\rho_{\pi(\theta)}}} \left[\nabla\pi(S;\theta)\left[\nabla_a q_{\pi(\theta)}(S,a)\right]_{a=\pi(S;\theta)}\right]
$$
其中的期望是针对折扣的状态分布（discounted state distribution）：
$$
\rho_\pi(s) = \int_{s_0 \in \mathcal S} {\large p}_{S_0}(s_0) \sum_{t=0}^{+\infty} \gamma^t {\rm Pr} \big[S_t=s \mid S_0=s_0;\theta \big] {\rm d}s_0
$$

而言的。证明：
$$
\begin{split}
& \nabla {\rm E}_{\pi_{(\theta)}}[G_0] = \sum_{t=0}^{+\infty}{\rm E}\left[\gamma^t\nabla\pi(S_t;\theta)\big[\nabla_a q_{\pi(\theta)}(S_t, a)\big]_{a=\pi(S_t;\theta)} \right] \\
&= \sum_{t=0}^{+\infty} \int_s {\large p}_{S_t}(s) \gamma^t \nabla\pi(s;\theta) \left[\nabla_a q_{\pi(\theta)}(s,a)\right]_{a=\pi({\color \red S_t};\theta)} {\rm d}s \\
&= \sum_{t=0}^{+\infty} \int_s \left(\int_{s_0}{\large p}_{S_0}(s_0) {\rm Pr}[S_t=s \mid S_0=s_0;\theta] {\rm d}s_0 \right) \gamma^t \nabla\pi(s;\theta) \left[\nabla_a q_{\pi(\theta)}(s,a)\right]_{a=\pi(s;\theta)} {\rm d}s \\
&= \int_s \left(\int_{s_0}{\large p}_{S_0}(s_0) \sum_{t=0}^{+\infty} \gamma^t {\rm Pr}[S_t=s \mid S_0=s_0;\theta] {\rm d}s_0 \right) \nabla\pi({\color \red S_t};\theta) \left[\nabla_a q_{\pi(\theta)}(s,a)\right]_{a=\pi(s;\theta)} {\rm d}s \\
&= \int_s \rho_{\pi(\theta)}(s) \nabla\pi(s;\theta) \left[\nabla_a q_{\pi(\theta)}(s,a)\right]_{a=\pi(s;\theta)} {\rm d}s \\
&= {\rm E}_{\large \rho_{\pi(\theta)}} \left[\nabla\pi(s;\theta) \left[\nabla_a q_{\pi(\theta)}(s,a)\right]_{a=\pi(s;\theta)} \right]
\end{split}
$$

*(在上式证明中，个人认为标红的项应该改为被积变量 $s$ 更为恰当；因为 $s$ 是随机变量 $S_t$ 的可能取值，在对 $S_t$ 求期望的积分表达式中，不应该出现随机变量 $S_t$ 。)*

### 一、同策确定性算法

根据策略梯度定理的确定性版本，将算法 8-1 中策略改进的迭代式和其他与策略相关的部分做相应修改，即可有连续动作空间中的确定性执行者 / 评论者算法。由于确定性策略输出确定，无法对环境进行充分探索，因此在训练的时候需要在策略的动作空间添加扰动实现探索，具体算法如下：
$$
\; \\ \; \\
\large \textbf{算法 9-1   基本的同策确定性执行者/评论者算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述）。} \\
&\text{输出：最优策略的估计 $\pi(\theta)$ 。} \\
&\text{参数：优化器（隐含学习率 $\alpha^{(\bold w)}, \alpha^{(\theta)}$ ），折扣因子 $\gamma$ ，控制回合数和回合内步数的参数。} \\
&\text{1.（初始化）$\theta \leftarrow$ 任意值，$\bold w \leftarrow$ 任意值。} \\
&\text{2.（带自益的策略更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化累计折扣）$I \leftarrow 1$ 。} \\
&\qquad \text{2.2（决定初始状态动作对）选择状态 $S$ ，对 $\pi(S; \theta)$ 加扰动（如正态分布随机变量扰动）得到动作 $A$ 。} \\
&\qquad \text{2.3 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.3.1（采样）根据状态 $S$ 和动作 $A$ 得到奖励 $R$ 和下一个状态 $S'$ ；} \\
&\qquad \qquad \text{2.3.2（执行）用 $\pi(S'; \theta)$ 加扰动得到 $A'$ ；} \\
&\qquad \qquad \text{2.3.3（估计回报）$U \leftarrow R + \gamma q(S',A';\bold w)$ ；} \\
&\qquad \qquad \text{2.3.4（更新价值）更新 $\bold w$ 以减小 $[U - q(S,A;\bold w)]^2$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\bold w \leftarrow \bold w + \alpha^{(\bold w)} [U - q(S,A;\bold w)] \nabla q(S,A;\bold w)$ ；} \\
&\qquad \qquad \text{2.3.5（策略改进）更新 $\theta$ 以减小 $-Iq(S,\pi(S;\theta);\bold w)$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\theta \leftarrow \theta + \alpha^{(\theta)} I\nabla\pi(S;\theta)[\nabla_a q(S,a;\bold w)]_{a=\pi(S;\theta)}$ ；} \\
&\qquad \qquad \text{2.3.6（更新累计折扣）$I \leftarrow \gamma I$ ；} \\
&\qquad \qquad \text{2.3.7（更新状态）$S \leftarrow S', A \leftarrow A'$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

在有些任务中，动作的效果经过低通滤波器处理后反映在系统中，而独立同分布的 Gaussian 噪声不能有效实现探索。例如在某个任务中，动作的直接效果是改变一个质点的加速度，如果在这个任务中用独立同分布的 Gaussian 噪声叠加在动作上，那么对质点位置的整体效果是在没有噪声的位置附近移动。这样的探索就没有办法为质点的位置提供持续的偏移，使得质点到比较远的位置。这类任务中，常常用 Ornstein Uhlenbeck 过程作为动作噪声，它是用下列随机微分方程定义的（以一维的情况为例）：
$$
{\rm d}N_t = \theta(\mu - N_t){\rm d}t + \sigma {\rm d}B_t
$$
其中 $\theta,\mu,\sigma$ 是参数（ $\theta>0,\sigma>0$ ），$B_t$ 是标准 Brownian 运动。当初始扰动是在原点的单点分布（即限定 $N_0=0$ ），并且 $\mu=0$ 时，上述方程的解为：
$$
N_t = \sigma \int_0^t e^{\theta(\tau-t)} {\rm d}B_t
$$
（证明：略）

这个解的均值为 0 ，方差为 $\displaystyle \frac{\sigma^2}{2\theta}(1-e^{-2\theta t})$ ，协方差为 $\displaystyle {\rm Cov}(N_t,N_s)=\frac{\sigma^2}{2\theta}(e^{-\theta|t-s|}-e^{-\theta(t+s)})$ ，（证明：略）。

对于 $t \ne s$ 总有 $|t-s|<t+s$ ，所以 ${\rm Cov}(N_t,N_s)>0$ ，据此可知，使用 Ornstein Uhlenbeck 过程让相邻扰动正相关，进而让动作向相近的方向偏移。

*(以上 Ornstein Uhlenbeck 过程的内容书中描述很少， 不好理解本质，所以证明省略了，后续需要再查阅相关资料。)*

### 二、异策确定性算法

确定性执行者 / 评论者算法也可以利用行为策略得到其异策版本。但对于确定性算法，行为策略并不对目标策略绝对连续，不再简单使用重采样。考虑非确定性版本的异策执行者 / 评论者算法的策略梯度可表示为：
$$
{\rm E}_b \left[\sum_{t=0}^{+\infty}\gamma^t \frac{\pi(A_t \mid S_t; \theta)}{b(A_t \mid S_t)} \Psi_t \nabla\ln\pi(A_t \mid S_t;\theta) \right] = {\rm E}_{\large \rho_b} \left[q_{\pi(\theta)}(S,A) \nabla\pi(A \mid S;\theta) \right]
$$
*（根据第八章的内容，上式中左边的 $\gamma^t$ 应该已经包含在 $\Psi_t$ 中；另外在该式的推导中，没有想明白 $\displaystyle \frac{1}{b(A_t \mid S_t)}$ 怎么消去的。）*

从这个角度看，可以通过最大化目标 ${\rm E}_{\large \rho_b}[q_{\pi(\theta)}(S,A) \pi(A \mid S;\theta)]$ 实现迭代。确定性版本的异策执行者 / 评论者算法从这个角度出发，试图最大化 ${\rm E}_{\large \rho_b}[q_{\pi(\theta)}(S,\pi(S;\theta))]$ ，那么其梯度为：
$$
\nabla {\rm E}_{\large \rho_b} [q_{\pi(\theta)}(S,\pi(S;\theta))] = {\rm E}_{\large \rho_b} \left[\nabla\pi(S;\theta) \big[\nabla_a q_{\pi(\theta)}(S,a)\big]_{a=\pi(S;\theta)} \right]
$$
这个表达式与同策的情形相比，期望运算针对的表达式相同，但是期望针对的分布不同，即行为策略确定了轨迹的分布，它体现在了期望上。由于行为策略能够促进探索，所以异策算法有时会比同策算法性能好。

基于以上，可以得到**异策确定性执行者 / 评论者算法**（Off-Policy Deterministic Actor-Critic, OPDAC）。由于在异策算法迭代更新策略参数时，对环境使用的是行为策略决定的动作，所以需要额外计算目标策略的动作；在更新价值函数时，采用的是 Q 学习，依然需要计算目标策略的动作。具体算法如下：

$$
\; \\ \; \\
\large \textbf{算法 9-2   基本的异策确定性执行者/评论者算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 9-1} \quad \cdots \\
&\text{2.2（初始状态）选择状态 $S$ 。} \\
&\text{2.3 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \text{2.3.1（执行）用 $b(\cdot \mid S)$ 得到 $A$ ；} \\
&\qquad \text{2.3.2（采样）根据状态 $S$ 和动作 $A$ 得到奖励 $R$ 和下一个状态 $S'$ ；} \\
&\qquad \text{2.3.3（估计回报）$U \leftarrow R + \gamma q(S',\pi(S';\theta);\bold w)$ ；} \\
&\cdots \quad \text{同算法 9-1} \quad \cdots \\
&\qquad \text{2.3.7（更新状态）$S \leftarrow S'$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

将基本的异策确定性执行者 / 评论者算法和深度 Q 网络中常用的技术（经验回放、目标网络）结合，即可得到**深度确定性策略梯度算法**（Deep Deterministic Policy Gradient, DDPG），算法 9-3 给出了该算法。另外，在更新目标网络时，为了避免参数更新过快，还引入了目标网络的学习率 $\alpha_{目标}\in(0,1)$ 。
$$
\; \\ \; \\
\large \textbf{算法 9-3   深度确定性策略梯度算法（假设 $\pi(S;\theta)+N$ 总是在动作空间内）} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述）。} \\
&\text{输出：最优策略的估计 $\pi(\theta)$ 。} \\
&\text{参数：学习率 $\alpha^{(\bold w)}, \alpha^{(\theta)}$ ，折扣因子 $\gamma$ ，控制回合数和回合内步数的参数，目标网络学习率 $\alpha_{目标}$ 。} \\
&\text{1.（初始化）$\theta \leftarrow$ 任意值，$\theta_{目标} \leftarrow \theta$ ，$\bold w \leftarrow$ 任意值，$\bold w_{目标} \leftarrow \bold w$ 。} \\
&\text{2. $\;\,$循环执行以下操作：} \\
&\qquad \text{2.1（积累经验）从起始状态 $S$ 出发，执行以下操作，直到满足终止条件：} \\
&\qquad \qquad \text{2.1.1 $\;\,$对 $\pi(S;\theta)$ 加扰动得到动作 $A$（如用正态分布随机变量扰动）；} \\
&\qquad \qquad \text{2.1.2 $\;\,$执行动作 $A$ ，观测到收益 $R$ 和下一状态 $S'$ ；} \\
&\qquad \qquad \text{2.1.3 $\;\,$将经验 $(S,A,R,S')$ 存储在经验存储空间 $\mathcal D$ 。} \\
&\qquad \text{2.2（更新）在更新的时机，执行一次或多次以下更新操作：} \\
&\qquad \qquad \text{2.2.1（回放）从存储空间 $\mathcal D$ 采样一批经验 $\mathcal B$ ；} \\
&\qquad \qquad \text{2.2.2（估计回报）$U \leftarrow R + \gamma q(S',\pi(S';\theta_{目标});\bold w_{目标})$ ，$(S,A,R,S')\in \mathcal B$ ；} \\
&\qquad \qquad \text{2.2.3（更新价值）更新 $\bold w$ 以减小 $\frac{1}{|\mathcal B|} \sum_{(S,A,R,S')\in \mathcal B} [U - q(S,A;\bold w)]^2$ ；} \\
&\qquad \qquad \text{2.2.4（策略改进）更新 $\theta$ 以减小 $-\frac{1}{|\mathcal B|} \sum_{(S,A,R,S')\in \mathcal B} q(S,\pi(S;\theta);\bold w)$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\theta \leftarrow \theta + \alpha^{(\theta)} \frac{1}{|\mathcal B|} \sum_{(S,A,R,S')\in \mathcal B} \nabla\pi(S;\theta)[\nabla_a q(S,a;\bold w)]_{a=\pi(S;\theta)}$ ；} \\
&\qquad \qquad \text{2.2.5（更新目标）在恰当的时机更新目标网络和目标策略，} \\
&\qquad \qquad \qquad \;\, \text{$\bold w_{目标} \leftarrow (1-\alpha_{目标}) \bold w_{目标} + \alpha_{目标}\bold w$ ，$\theta_{目标} \leftarrow (1-\alpha_{目标}) \theta_{目标} + \alpha_{目标}\theta$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
S. Fujimoto 等人在文章《Addressing function approximation error in actor-critic methods》中给出了**双重延迟深度确定性策略梯度算法**（Twin Delay Deep Deterministic Policy Gradient, TD3），结合了深度确定性策略梯度算法和双重 Q 学习。

对于确定性策略梯度算法，动作已经由含参策略 $\pi(\theta)$ 决定了，双重网络则要由双重延迟深度确定性策略梯度算法维护两份学习过程的价值网络参数 $\bold w^{(i)}$ 和目标网络参数 $\bold w_{目标}^{(i)}$ （$i=0,1$）。在估计目标时，选取两个目标网络得到的结果中较小的那个，即 $\displaystyle \min_{i=0,1}q\left(\cdot, \cdot;\bold w_{目标}^{(i)} \right)$ ，具体算法如下：
$$
\; \\ \; \\
\large \textbf{算法 9-4   双重延迟深度确定性策略梯度算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 9-3} \quad \cdots \\
&\text{1.（初始化）$\theta \leftarrow$ 任意值，$\theta_{目标} \leftarrow \theta$ ，$\bold w^{(i)} \leftarrow$ 任意值，$\bold w_{目标}^{(i)} \leftarrow \bold w^{(i)}$ ，$i \in \{0,1\}$ 。} \\
&\cdots \quad \text{同算法 9-3} \quad \cdots \\
&\qquad \qquad \text{2.3.2（扰动动作）为目标动作 $\pi(S';\theta_{目标})$ 加受限的扰动，得到动作 $A'$ ，$(S,A,R,S')\in \mathcal B$ ；} \\
&\qquad \qquad \text{2.3.3（估计回报）$U \leftarrow R + \gamma \min_{i=0,1} q\left(S',A';\bold w^{(i)}\right)$ ，$(S,A,R,S')\in \mathcal B$ ；} \\
&\qquad \qquad \text{2.3.4（更新价值）更新 $\bold w^{(i)}$ 以减小 $\frac{1}{|\mathcal B|} \sum_{(S,A,R,S')\in \mathcal B} \left[U - q\left(S,A;\bold w^{(i)}\right)\right]^2$ ，$(i = 0,1)$ ；} \\
&\qquad \qquad \text{2.3.5（策略改进）更新 $\theta$ 以减小 $-\frac{1}{|\mathcal B|} \sum_{(S,A,R,S')\in \mathcal B} q\left(S,\pi(S;\theta);\bold w^{(0)}\right)$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\theta \leftarrow \theta + \alpha^{(\theta)} \frac{1}{|\mathcal B|} \sum_{(S,A,R,S')\in \mathcal B} \nabla\pi(S;\theta) \left[\nabla_a q(S,a;\bold w^{(0)})\right]_{a=\pi(S;\theta)}$ ；} \\
&\qquad \qquad \text{2.3.6（更新目标）在恰当的时机更新目标网络和目标策略，$\bold w_{目标}^{(i)} \leftarrow (1-\alpha_{目标}) \bold w_{目标}^{(i)}$} \\
&\qquad \qquad \qquad \;\, \text{$+ \alpha_{目标}\bold w^{(i)}（i=0,1）$ ，$\theta_{目标} \leftarrow (1-\alpha_{目标}) \theta_{目标} + \alpha_{目标}\theta$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

### 三、案例：倒立摆控制（Pendulum-v0）

本节使用 Gym 库中的倒立摆控制问题（Pendulum-v0）。该问题的对象是一根一端固定的棍子，以固定点为原点，垂直向上为 $x$ 轴方向，水平向右为 $y$ 轴方向；观测值为棍子活动端的坐标 $(X_t,Y_t)=(\cos\Theta_t,\sin\Theta_t),\;\Theta_t\in[-\pi,\pi)$ 和角速度 $\dot{\Theta}_t \in [-8,+8]$ ；动作为连续值，是一个施加在活动端的力矩 $A_t \in [-2,+2]$ ；奖励值也是一个和状态与动作有关的连续值 $R_{t+1} \in [-\pi^2-6.404, 0]$ ；任务是在给定的时间内（200 步）总收益越大越好，即相当于尽可能的保持木棍静止直立。其他更详细的数据可参阅[源代码][1]。

该问题的状态空间、动作空间、奖励空间都是连续的空间，问题整体的空间变得相当大；而且最大的力矩也无法将倒立摆从单边直接推至直立状态，因此智能体需要学会利用重力将倒立摆荡上至直立位置。

代码中类 `OrnsteinUhlenbeckProcess` 实现了 Ornstein Uhlenbeck 过程；智能体类 `DDPG` 实现了深度确定性策略梯度智能体类，以及智能体类 `TD3` 实现了双重延迟深度确定性策略梯度智能体类；前两者代码与书中基本一致，后者稍作修改，减少了一些代码量，但实现逻辑不变，此处不再展示。

[1]: https://github.com/openai/gym/blob/master/gym/envs/classic_control/pendulum.py	"Pendulum-v0"