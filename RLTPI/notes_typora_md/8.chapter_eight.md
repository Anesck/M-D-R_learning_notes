## 第八章：执行者 / 评论者方法

将策略梯度算法和自益相结合：一方面，用含参函数近似价值函数，然后利用这个价值函数的近似值来估计回报值；另一方面，利用估计得到的回报值估计策略梯度，进而更新策略参数。这两方面又被称为**评论者**（critic）和**执行者**（actor），所以该类算法被称为**执行者 / 评论者算法**（actor-critic algorithm）。

### 一、同策执行者 / 评论者算法

执行者 / 评论者算法同样使用动作偏好函数，以及取 $E[\Psi_t\nabla\ln\pi(A_t \mid S_t;\theta)]$ 为梯度方向进行迭代更新，其中 $\Psi_t = \gamma^t(G_t-B(s))$ 。J. Schulman 等在文章《High-dimensional continuous control using generalized advantage estimation》中指出，$\Psi_t$ 并不拘泥于以上形式，还可以为以下几种形式：

- （动作价值）$\Psi_t = \gamma^tq_\pi(S_t,A_t)$ ；
- （优势函数）$\Psi_t = \gamma^t[q_\pi(S_t,A_t)-v_\pi(S_t)]$ ；
- （时序差分）$\Psi_t = \gamma^t[R_{t+1} + \gamma v_\pi(S_{t+1})-v_\pi(S_t)]$ ；

其中，前两者是使用价值函数来估计回报，而时序差分则是用 $R_{t+1}+\gamma v_\pi(S_{t+1})$ 代表回报，再减去基线 $B(s)=v_\pi(s)$ 以减小方差。但由于实际使用时的真实价值函数是不知道的，所以采用自益的方法用价值估计 $U_t$ 作为回报，即 $\Psi_t = \gamma^t[R_{t+1} + \gamma v_\pi(S_{t+1};\bold w)-v_\pi(S_t;\bold w)]$ ，这里估计值 $v(\bold w)$ 就是评论者。另外，只有采用了自益的方法，即用价值估计来估计回报，并引入了偏差，才是执行者 / 评论者算法；用价值估计来做基线并没有带来偏差，所以带基线的简单策略梯度算法不属于执行者 / 评论者算法。

当使用 $q_\pi(S_t,A_t;\bold w)$ 作为回报估计，并取 $\Psi_t = \gamma^tq_\pi(S_t,A_t;\bold w)$ 时的同策算法称为动作价值执行者 / 评论者算法：
$$

$$
当使用 $q_\pi(S_t,A_t;\bold w)$ 作为回报估计，并取 $\Psi_t = \gamma^t[q_\pi(S_t,A_t;\bold w)-v_\pi(S_t;\bold w)]$ 时的同策算法称为优势执行者 / 评论者算法，
$$

$$
执行者 / 评论者算法能够和资格迹结合，算法 8-4 给出了带资格迹的优势执行者 / 评论者算法。
$$

$$

### 二、基于代理优势的同策算法

在迭代过程中不直接优化期望目标，而是试图优化期望目标的近似——代理优势，这类算法称为面向代理优势的执行者 / 评论者算法。在很多问题上，这些算法会比简单的执行者 / 评论者算法得到更好的性能。



### 三、信任域算法



### 四、重要性采样异策执行者 / 评论者算法



### 五、柔性执行者 / 评论者算法



### 六、案例：双节倒立摆（Acrobot-v1）



