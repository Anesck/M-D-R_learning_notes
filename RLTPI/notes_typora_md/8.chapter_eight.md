## 第八章：执行者 / 评论者方法

将策略梯度算法和自益相结合：一方面，用含参函数近似价值函数，然后利用这个价值函数的近似值来估计回报值；另一方面，利用估计得到的回报值估计策略梯度，进而更新策略参数。这两方面又被称为**评论者**（critic）和**执行者**（actor），所以该类算法被称为**执行者 / 评论者算法**（actor-critic algorithm）。

### 一、同策执行者 / 评论者算法

执行者 / 评论者算法同样使用含参函数  $h(s,a;\theta)$ 表示偏好，以及取 $E[\Psi_t\nabla\ln\pi(A_t \mid S_t;\theta)]$ 为梯度方向进行迭代更新，其中 $\Psi_t = \gamma^t(G_t-B(s))$ 。J. Schulman 等人在文章《High-dimensional continuous control using generalized advantage estimation》中指出，$\Psi_t$ 并不拘泥于以上形式，还可以为以下几种形式：

- （动作价值）$\Psi_t = \gamma^tq_\pi(S_t,A_t)$ ；
- （优势函数）$\Psi_t = \gamma^t[q_\pi(S_t,A_t)-v_\pi(S_t)]$ ；
- （时序差分）$\Psi_t = \gamma^t[R_{t+1} + \gamma v_\pi(S_{t+1})-v_\pi(S_t)]$ ；

其中，前两者是使用价值函数 $q_\pi(S_t,A_t)$ 来估计回报的，而时序差分则是用 $R_{t+1}+\gamma v_\pi(S_{t+1})$ 代表回报，再减去基线函数 $B(s)=v_\pi(s)$ 以减小学习过程中的方差。但由于实际使用时的真实价值函数是不知道的，所以采用自益的方法用价值估计 $U_t$ 作为回报，即 $\Psi_t = \gamma^t[R_{t+1} + \gamma v_\pi(S_{t+1};\bold w)-v_\pi(S_t;\bold w)]$ ，这里估计值 $v(\bold w)$ 就是评论者。另外，只有采用了自益的方法，即用价值估计来估计回报，并引入了偏差，才是执行者 / 评论者算法；用价值估计来做基线并没有带来偏差，所以带基线的简单策略梯度算法不属于执行者 / 评论者算法。

当使用 $q(S_t,A_t;\bold w)$ 作为回报估计，并取 $\Psi_t = \gamma^tq(S_t,A_t;\bold w)$ 时的同策算法称为动作价值执行者 / 评论者算法：
$$
\; \\ \; \\
\large \textbf{算法 8-1   动作价值同策执行者/评论者算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\text{输入：环境（无数学描述）。} \\
&\text{输出：最优策略的估计 $\pi(\theta)$ 。} \\
&\text{参数：优化器（隐含学习率 $\alpha^{(\bold w)}, \alpha^{(\theta)}$ ），折扣因子 $\gamma$ ，控制回合数和回合内步数的参数。} \\
&\text{1.（初始化）$\theta \leftarrow$ 任意值，$\bold w \leftarrow$ 任意值。} \\
&\text{2.（带自益的策略更新）对于每个回合执行以下操作：} \\
&\qquad \text{2.1（初始化累计折扣）$I \leftarrow 1$ 。} \\
&\qquad \text{2.2（决定初始状态动作对）选择状态 $S$ ，并用 $\pi(\cdot \mid S; \theta)$ 得到动作 $A$ 。} \\
&\qquad \text{2.3 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \qquad \text{2.3.1（采样）根据状态 $S$ 和动作 $A$ 得到奖励 $R$ 和下一个状态 $S'$ ；} \\
&\qquad \qquad \text{2.3.2（执行）用 $\pi(\cdot \mid S'; \theta)$ 得到 $A'$ ；} \\
&\qquad \qquad \text{2.3.3（估计回报）$U \leftarrow R + \gamma q(S',A';\bold w)$ ；} \\
&\qquad \qquad \text{2.3.4（策略改进）更新 $\theta$ 以减小 $-Iq(S,A;\bold w) \ln \pi(A \mid S; \theta)$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\theta \leftarrow \theta + \alpha^{(\theta)} Iq(S,A;\bold w) \nabla \ln \pi(A \mid S; \theta)$ ；} \\
&\qquad \qquad \text{2.3.5（更新价值）更新 $\bold w$ 以减小 $[U - q(S,A;\bold w)]^2$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\bold w \leftarrow \bold w + \alpha^{(\bold w)} [U - q(S,A;\bold w)] \nabla q(S,A;\bold w)$ ；} \\
&\qquad \qquad \text{2.3.6（更新累计折扣）$I \leftarrow \gamma I$ ；} \\
&\qquad \qquad \text{2.3.7（更新状态）$S \leftarrow S', A \leftarrow A'$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
当使用 $q(S_t,A_t;\bold w)$ 作为回报估计，并取 $\Psi_t = \gamma^t[q_(S_t,A_t;\bold w)-v(S_t;\bold w)]$ 时的同策算法称为优势执行者 / 评论者算法；相当于在基本的执行者 / 评论者算法中引入基线函数 $B(S_t)=v(S_t;\bold w)$ 得到的，其中 $q(S_t,A_t;\bold w)-v(S_t;\bold w)$ 是优势函数的估计。由于采用 $q(S_t,A_t;\bold w)-v(S_t;\bold w)$ 估计优势函数需要两个函数分别表示 $q(\bold w)$ 和 $v(\bold w)$ ，为了简化，可以使用 $U_t = R_{t+1} + \gamma v(S_{t+1};\bold w)$ 作为目标，这样优势函数的估计就变为单步时序差分的形式 $R_{t+1} + \gamma v(S_{t+1};\bold w) - v(S_t;\bold w)$ 。

如果优势执行者 / 评论者算法在执行过程中不是每一步都更新参数，而是在回合结束后用整个轨迹来进行更新，就可以把算法分为经验搜集和经验使用两个部分，这样的分隔可以让该算法同时有很多执行者在同时执行。每个执行者在一定的时机更新参数，同时更新策略参数 $\theta$ 和价值参数 $\bold w$ ，每个执行者的更新都是异步的，所以这样的并行算法称为**异步优势执行者 / 评论者算法**（Asynchronous Advantage Actor-Critic, A3C）。异步优势执行者 / 评论者算法中的自益部分，不仅可以采用单步时序差分，也可以使用多步时序差分；另外，还可以对函数参数的访问进行控制，使得所有执行者统一更新参数，这样的并行算法称为**优势执行者 / 评论者算法**（Advantage Actor-Critic, A2C）。

算法 8-2 和算法 8-3 分别给出了这两个算法*（书中给出的 8-2 优势执行者 / 评论者算法似乎不是上面所提到的并行算法）*：
$$
\; \\ \; \\
\large \textbf{算法 8-2   优势执行者/评论者算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 8-1} \quad \cdots \\
&\text{2.2（决定初始状态）选择状态 $S$ 。} \\
&\text{2.3 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \text{2.3.1（采样）用 $\pi(\cdot \mid S; \theta)$ 得到动作 $A$ ；} \\
&\qquad \text{2.3.2（执行）执行动作 $A$ ，得到奖励 $R$ 和观测 $S'$ ；} \\
&\qquad \text{2.3.3（估计回报）$U \leftarrow R + \gamma v(S';\bold w)$ ；} \\
&\qquad \text{2.3.4（策略改进）更新 $\theta$ 以减小 $-I[U-v(S;\bold w)] \ln \pi(A \mid S; \theta)$ ，} \\
&\qquad \qquad \;\, \text{如 $\theta \leftarrow \theta + \alpha^{(\theta)} I[U-v(S;\bold w)] \nabla \ln \pi(A \mid S; \theta)$ ；} \\
&\qquad \text{2.3.5（更新价值）更新 $\bold w$ 以减小 $[U - v(S;\bold w)]^2$ ，如 $\bold w \leftarrow \bold w + \alpha^{(\bold w)} [U - v(S;\bold w)] \nabla v(S;\bold w)$ ；} \\
&\qquad \text{2.3.6（更新累计折扣）$I \leftarrow \gamma I$ ；} \\
&\qquad \text{2.3.7（更新状态）$S \leftarrow S'$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
$$
\; \\ \; \\
\large \textbf{算法 8-3   异步优势执行者/评论者算法（演示某个线程的行为）} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 8-1} \quad \cdots \\
&\text{1.（同步全局参数）$\theta' \leftarrow \theta$ ，$\bold w' \leftarrow \bold w$ 。} \\
&\text{2. $\;\,$逐回合执行以下过程：} \\
&\qquad \text{2.1 $\;\,$用策略 $\pi(\theta')$ 生成轨迹 $S_0,A_0,R_1,S_1,\cdots,S_{T-1},A_{T-1},R_T,S_T$ 直到回合结束或达步数上限。} \\
&\qquad \text{2.2 $\;\,$为梯度计算初始化：} \\
&\qquad \qquad \text{2.2.1（初始化目标 $U_T$）若 $S_T$ 是终止状态，则 $U \leftarrow 0$ ，否则 $U \leftarrow v(S_T;\bold w')$ ；} \\
&\qquad \qquad \text{2.2.2（初始化梯度） $\bold g^{(\theta)} \leftarrow 0$ ，$\bold g^{(\bold w)} \leftarrow 0$ 。} \\
&\qquad \text{2.3（异步计算梯度）对 $t=T-1,T-2,\cdots,0$ ，执行以下操作：} \\
&\qquad \qquad \text{2.3.1（估计回报）$U \leftarrow R_{t+1} + \gamma U$ ；} \\
&\qquad \qquad \text{2.3.2（估计策略梯度）$\bold g^{(\theta)} \leftarrow \bold g^{(\theta)} + [U - v(S_t;\bold w')] \nabla \ln \pi(A_t \mid S_t; \theta')$ ；} \\
&\qquad \qquad \text{2.3.5（估计价值梯度）$\bold g^{(\bold w)} \leftarrow \bold g^{(\bold w)} + [U - v(S_t;\bold w')] \nabla v(S_t;\bold w')$ ；} \\
&\text{3.（同步更新）更新全局参数：} \\
&\qquad \text{3.1（策略更新）用 $\bold g^{(\theta)}$ 更新全局策略参数 $\theta$ ，如 $\theta \leftarrow \theta + \alpha^{(\theta)} \bold g^{(\theta)}$ ；} \\
&\qquad \text{3.2（价值更新）用 $\bold g^{(\bold w)}$ 更新全局价值参数 $\bold w$ ，如 $\bold w \leftarrow \bold w + \alpha^{(\bold w)} \bold g^{(\bold w)}$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
（原作者注：算法 8-3 的步骤 2.3.2 中没有考虑累计折扣是为了遵循论文原文。）

执行者 / 评论者算法能够和资格迹结合，与之前函数近似中使用资格迹的方法类似，这里的资格迹也是给对应的函数近似价值参数进行加权更新的。算法 8-4 给出了带资格迹的优势执行者 / 评论者算法。
$$
\; \\ \; \\
\large \textbf{算法 8-4   带资格迹的优势执行者/评论者算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 8-1，但加两个资格迹参数： $\lambda^{(\theta)}, \lambda^{(\bold w)}$ 。} \quad \cdots \\
&\text{2.1（初始化资格迹和累计折扣）$\bold z^{(\theta)} \leftarrow 0$ ，$\bold z^{(\bold w)} \leftarrow 0$ ，$I \leftarrow 1$ 。} \\
&\cdots \quad \text{同算法 8-2} \quad \cdots \\
&\qquad \text{2.3.4（更新策略资格迹）$\bold z^{(\theta)} \leftarrow \gamma \lambda^{(\theta)} \bold z^{(\theta)} + I\nabla \ln\pi(A \mid S; \bold w)$ ；} \\
&\qquad \text{2.3.5（策略改进）$\theta \leftarrow \theta + \alpha^{(\theta)} [U-v(S;\bold w)] \bold z^{(\theta)}$ ；} \\
&\qquad \text{2.3.6（更新价值资格迹）$\bold z^{(\bold w)} \leftarrow \gamma \lambda^{(\bold w)} \bold z^{(\bold w)} + \nabla v(S; \bold w)$ ；} \\
&\qquad \text{2.3.7（更新价值）$\bold w \leftarrow \bold w + \alpha^{(\bold w)} [U - v(S;\bold w)] \bold z^{(\bold w)}$ ；} \\
&\cdots \quad \text{同算法 8-2} \quad \cdots \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$

### 二、基于代理优势的同策算法

在迭代过程中不直接优化期望目标，而是试图优化期望目标的近似——代理优势，这类算法称为面向代理优势的执行者 / 评论者算法。在很多问题上，这些算法会比简单的执行者 / 评论者算法得到更好的性能。

暂略

### 三、信任域算法

暂略

### 四、重要性采样异策执行者 / 评论者算法

执行者 / 评论者算法可以和重要性采样结合，得到**异策的执行者 / 评论家算法**（Off-Policy Actor-Critic, OffPAC）。用 $b(\cdot \mid \cdot)$ 表示行为策略，则梯度方向可由 ${\rm E}_{\pi(\theta)}[\Psi_t \nabla\ln\pi(A_t \mid S_t;\theta)]$ 变为 $\displaystyle {\rm E}_b \left[\frac{\pi(A_t \mid S_t;\theta)}{b(A_t \mid S_t)} \Psi_t \nabla\ln\pi(A_t \mid S_t;\theta) \right]$$\displaystyle ={\rm E}_b \left[\frac{1}{b(S_t \mid A_t)} \Psi_t \nabla\pi(A_t \mid S_t;\theta) \right]$ 。这时，更新策略参数 $\theta$ 时就应该试图减小 $\displaystyle -\frac{1}{b(A_t \mid S_t)} \Psi_t \nabla\pi(A_t \mid S_t;\theta)$ ，具体算法如下：
$$
\; \\ \; \\
\large \textbf{算法 8-10   异策动作价值执行者/评论者算法} \\
\begin{split}
\rule[5pt]{10mm}{0.1em} &\rule[5pt]{265mm}{0.1em} \\
&\cdots \quad \text{同算法 8-1} \quad \cdots \\
&\text{2.2（决定初始状态动作对）选择状态 $S$ ，并用 $b(\cdot \mid S)$ 得到动作 $A$ 。} \\
&\text{2.3 $\;\,$如果回合未结束，执行以下操作：} \\
&\qquad \text{2.3.1（采样）根据状态 $S$ 和动作 $A$ 得到奖励 $R$ 和下一个状态 $S'$ ；} \\
&\qquad \text{2.3.2（执行）用 $b(\cdot \mid S')$ 得到 $A'$ ；} \\
&\qquad \text{2.3.3（估计回报）$U \leftarrow R + \gamma q(S',A';\bold w)$ ；} \\
&\qquad \text{2.3.4（策略改进）更新 $\theta$ 以减小 $-\frac{1}{b(A \mid S)}Iq(S,A;\bold w) \pi(A \mid S; \theta)$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\theta \leftarrow \theta + \alpha^{(\theta)} \frac{1}{b(A \mid S)}Iq(S,A;\bold w) \nabla\pi(A \mid S; \theta)$ ；} \\
&\qquad \qquad \text{2.3.5（更新价值）更新 $\bold w$ 以减小 $\frac{\pi(A \mid S; \theta)}{b(A \mid S)}[U - q(S,A;\bold w)]^2$ ，} \\
&\qquad \qquad \qquad \;\, \text{如 $\bold w \leftarrow \bold w + \alpha^{(\bold w)} \frac{\pi(A \mid S; \theta)}{b(A \mid S)} [U - q(S,A;\bold w)] \nabla q(S,A;\bold w)$ ；} \\
&\qquad \qquad \text{2.3.6（更新累计折扣）$I \leftarrow \gamma I$ ；} \\
&\qquad \qquad \text{2.3.7（更新状态）$S \leftarrow S', A \leftarrow A'$ 。} \\
\rule[-5pt]{10mm}{0.1em} &\rule[-5pt]{265mm}{0.1em}
\end{split}
\; \\ \; \\
$$
Z. Wang 等在文章《Sample efficient actor-critic with experience replay》中提出了**带经验回放的执行者 / 评论者算法**（Actor-Critic with Experiment Replay, ACER）。

暂略

### 五、柔性执行者 / 评论者算法

暂略

### 六、案例：双节倒立摆（Acrobot-v1）

本节使用 gym 库中的双节倒立摆（Acrobot-v1）作为实验对象。该对象由两节杆子连接组成，并将其中一根杆子未连接的一端固定住，环境的状态和动作取值在[源代码][1]中都有说明；奖励则是每执行一个动作返回 -1 ，回合结束时的奖励值为 0 ；当回合步数达到 500 或活动端（未固定的那节杆子）在垂直方向上高于原点（固定点）至少一节杆子的高度时，该回合结束；目的是以尽可能少的步数结束回合。另外，该对象的动力学方程式已知的，但非常复杂，即使知道该表达式，也不可能求出最优动作的闭式解。

代码中的智能体类 `QAC` 实现了动作价值执行者 / 评论者类，与书中代码基本一致。 `AdvantageAC` 实现了优势执行者 / 评论者类，稍微改写了下，使之继承于 `QAC` 。`ElibilityTraceAC` 实现了带资格迹的优势执行者 / 评论者类，但书中源代码在计算评论者的资格迹时，可能是由复制粘贴造成的疏忽，将 $\nabla v(S;\bold w)$ 乘以了折扣 $I$ ；根据算法 8-4 ，折扣 $I$ 只出现在行动者的资格迹计算中，因此将该操作按算法纠正了；不过经简单的两次测试发现，是否纠正似乎对结果影响并不明显。具体代码此处不再展示。

其他算法暂略

[1]: https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py	"Acrobot-v1"

