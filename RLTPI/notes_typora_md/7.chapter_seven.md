## 第七章：回合更新策略梯度方法

在前几章的算法中，求解最优策略都是试图估计最优价值函数，这些算法称为**最优价值算法**（optimal value algorithm）。本章开始介绍试图用含参函数近似最优策略，并通过迭代更新参数值，这类算法称为**策略梯度算法**（optimal gradient algorithm）。

### 一、策略梯度原理

用函数近似方法估计最优策略 $\pi(a \mid s)$ 的基本思想是用含参函数 $\pi(a \mid s; \Bbb\theta)$ 来近似最优策略，由于任意策略都需要满足对于任意的状态 $s \in \mathcal S$ ，均有 $\displaystyle \sum_a \pi(a \mid s) = 1$ 。为此引入**动作偏好函数**（action preference function）$h(s,a;\theta)$ ，其 softmax 的值为 $\pi(a \mid s; \theta)$ ，即：
$$
\pi(a \mid s; \theta) = \frac{exp h(s,a;\theta)}{\sum_{a'}exp h(s,a';\theta)}\; , \qquad s \in \mathcal S, a \in \mathcal A
$$


### 二、同策回合更新策略梯度算法



### 三、异策回合更新策略梯度算法



### 四、策略梯度更新与极大似然估计的关系



### 五、车杆平衡（CartPole-v0）





